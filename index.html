<!-- --> 
<!-- saved from url=(0036)http://appsrv.cse.cuhk.edu.hk/~xchu/ -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><HTML 
xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><META 
content="IE=11.0000" http-equiv="X-UA-Compatible">

<META name="GENERATOR" content="MSHTML 11.00.9600.18125">
<META http-equiv="Content-Type" content="text/html;charset=utf-8">
<LINK href="style/jemdoc.css" rel="stylesheet" type="text/css">
<TITLE>Dongchao Yang</TITLE>
</HEAD>

<BODY>
<!--<div id="particles-js" class="particles-js"></div>-->
<!--<script src="./particles.js"></script>-->
<!--<script src="./app.js"></script>-->
<DIV id="layout-content">
<DIV id="make-narrow">
<DIV id="toptitle">

<H1>Dongchao Yang</H1></DIV>

<P>
<img src="img/dongchao.jpg" align="right" style="width:200px;height:210px;"> 
</P>

<P>School of Electronic and Computer Engineering,<BR> Peking University, China</P>
<P><B>Email</B>: dongchao98@stu.pku.edu.cn</P>
<P><B>Phone</B>: +8615087581161</P>
<!-- <P><B>WeChat</B>: wxidbhl</P> -->
<p> 
<a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AJsN-F4czAkBmd4DO5WeMWN__2LqYg8Jay5asaDiB_UIfjsFyeVUj51geK3oGFEkutPWQ5_UGK3A0gZcH7mWzlAEoL09PJn3MUctPS9dKIedWhwSgg_mVfs&user=WNiojyAAAAAJ"><img src="./img/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
<a href="https://github.com/yangdongchao"><img src="./img/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
</p>
<!-- <P><A href="assets/cv.pdf">[Curriculum Vitae]</A></P> -->
<!-- <P><A href="https://dblp.uni-trier.de/pers/hd/b/Bai:Haoli">[DBLP]</A> -->
<!-- <A href="https://github.com/haolibai">[GitHub]</A> -->
<!--<a href="http://appsrv.cse.cuhk.edu.hk/~hlbai/CV_Haoli.pdf">[CV]</A>-->



<H2>General</H2>
<!--    <P>I am a third-year PhD student at Peking University, majoring in Speech and Audio Processing.<BR> Before that, I received the Bachelor's Degree from Tsinghua University in 2019. <BR>-->
    <P>I am a second-year master student at Peking University, majoring in Speech and Audio Processing.<BR> Before that, I received the Bachelor's Degree from Shanghai University in 2020. <BR>(<B>Note:</B> I will graduate in July 2023, and I am actively looking for <B>PhD opportunities</B> and <B>industrial opportunities</B> starting from <B>2023 Fall</B>. Please feel free to contact me.)

<H2>Research Interests</H2>
<P>Machine Learning, Audio Processing, Speech Processing</P>

<H2>Educations</H2>
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"></div>
  </div>

<UL>
  <LI>
    <div style="float:left;">School of Electronic and Computer Engineering, <A href="http://www.pku.edu.cn/"> Peking University</A></div>
    <div style="float:right;">August 2020 - Now.</div>
  </LI>
</UL>

<UL>
  <LI>
    <div style="float:left;">School of Computer Engineering and Science, <A href="https://cs.shu.edu.cn/index.htm"> Shanghai University</A></div>
    <div style="float:right;">August 2016 - July 2020.</div>
  </LI>
</UL>

<H2>Experiences</H2>
<!--<UL>-->
<!--  <LI>-->
<!--  <P>May 2021 - Now<BR>-->
<!--      <B>National University of Singapore</B>, Human Language Technology, Summer Research. <BR>(Supervised by <A href="https://scholar.google.com.sg/citations?user=z8_x7C8AAAAJ"> Haizhou Li</A>)-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->
<!-- <A href="https://scholar.google.com/citations?user=OSM9xooAAAAJ"> Yi Luo </A> -->
<UL>
  <LI>
  <P>May 2021 - Now<BR>
      <B>Tencent AI Lab</B>, <A href="https://ai.tencent.com/ailab/en/index"> Speech Group</A>, Intern. <BR> Supervisor: <A href="https://scholar.google.com/citations?user=s62BKJIAAAAJ"> Bo Wu </A>, and <A href="https://scholar.google.com/citations?user=pRA19-8AAAAJ"> Chao Weng</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>August 2020 - Now<BR>
      <B>Peking University</B>, <A href="https://web.pkusz.edu.cn/adsp/"> ADSP Lab</A>, master student. <BR> Supervisor: <A href="https://scholar.google.com/citations?user=sfyr7zMAAAAJ"> Yuexian Zou</A> <BR> Co-author: <A href="https://scholar.google.co.uk/citations?user=JQFnV5IAAAAJ"> Wenwu Wang</A>
   </P>
  </LI>
</UL>


<H2>Publications</H2>
<!-- <p>(Conference: 3 & Journal: 0)</p> -->

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Helin Wang, Yuexian Zou <BR>
    <A href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/yang21b_interspeech.pdf"> Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification </A><BR>
    <I>Proc. Interspeech</I>, 2021.
    <A href="https://github.com/yangdongchao/interspeech2021_MTDA">[Code]</A>
   </P>
  </LI>
</UL>
<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Helin Wang, Yuexian Zou, Zhongjie Ye, WenWu Wang <BR>
    <A href="https://arxiv.org/pdf/2110.04474.pdfhttps://arxiv.org/pdf/2110.04474.pdf"> A MUTUAL LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION
    </A><BR>
    <I>ICASSP</I>, 2022.
    <A href="https://github.com/yangdongchao/DCASE2021Task5">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Helin Wang, Zhongjie Ye, Yuexian Zou <BR>
    <A href="http://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Zou_22_task5.pdf"> Few-shot Bioacoustic Event Detection: A Good Transductive Inference is All You Need </A><BR>
    <I>Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE)</I>, 2021.
      <A href="https://github.com/yangdongchao/DCASE2021Task5">[Code]</A>
   </P>
  </LI>
</UL>


<UL>
  <LI>
  <P><B>Dongchao Yang \*</B>, <B>Helin Wang \*</B>, Zhongjie Ye, Yuexian Zou, WenWu Wang <BR>
    <A href="https://arxiv.org/pdf/2204.02143.pdf"> RaDur: A Reference-aware and Duration-robust Network for Target Sound
      Detection </A><BR>
    <I>Submitted to Interspeech 2022</I>, 2021.
      <A href="https://github.com/yangdongchao/RaDur">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, <B>Helin Wang</B>, Yuexian Zou, WenWu Wang <BR>
    <A href="https://arxiv.org/pdf/2204.02088.pdf"> A Two-student Learning Framework for Mixed Supervised Target Sound Detection </A><BR>
    <I>Submitted to Interspeech 2022</I>, 2021.
      <A href="https://github.com/yangdongchao/weakly-target-sound-detection">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B> Helin Wang</B>, <B>Dongchao Yang </B>, Yuexian Zou <BR>
    <A href="https://arxiv.org/pdf/2112.10153.pdf"> Detect what you want: Target Sound Detection </A><BR>
    <I>Submitted to Interspeech 2022</I>, 2021.
      <A href="https://github.com/yangdongchao/Target-sound-event-detection">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B> Helin Wang</B>, <B> Dongchao Yang </B>, Chao Weng, Jianwei Yu, Yuexian Zou <BR>
    <A href="https://arxiv.org/pdf/2204.00821.pdf"> Improving Target Sound Extraction with Timestamp Information </A><BR>
    <I>Submitted to Interspeech 2022</I>, 2021.
      <A href="https://github.com/yangdongchao/Tim-TSENet">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>Zhongjie Ye, Helin Wang, <B>Dongchao Yang</B>, Yuexian Zou <BR>
    <A href="https://arxiv.org/pdf/2110.06100.pdf"> Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information </A><BR>
    <I>DCASE2021 Workshop</I>, 2021.
      <A href="https://github.com/WangHelin1997/DCASE2021_Task6_PKU">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>Zhongjie Ye, Helin Wang, <B>Dongchao Yang</B>, Yuexian Zou <BR>
    <A href="http://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Zou_22_task5.pdf"> Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Textual Information </A><BR>
    <I>Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE)</I>, 2021.
      <A href="https://github.com/WangHelin1997/DCASE2021_Task6_PKU">[Code]</A>
   </P>
  </LI>
</UL>
<!--<H2>Preprints</H2>-->
<!--&lt;!&ndash; <p>(Conference: 3 & Journal: 0)</p> &ndash;&gt;-->
<!--<UL>-->
<!--  <LI>-->
<!--  <P><B>Helin Wang</B>, Yuexian Zou, Wenwu Wang <BR>-->
<!--      SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification <BR>-->
<!--      Submitted to Interspeech 2021.-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->
<!--<UL>-->
<!--  <LI>-->
<!--  <P><B>Helin Wang</B>, Bo Wu, Lianwu Chen, Meng Yu, Jianwei Yu, Yong Xu, Shi-Xiong Zhang, Chao Weng, Dan Su, Dong Yu <BR>-->
<!--      TeCANet: Temporal-Contextual Attention Network for Environment-aware Speech Dereverberation <BR>-->
<!--      Submitted to Interspeech 2021.-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->
<!--<UL>-->
<!--  <LI>-->
<!--  <P>Dongchao Yang, <B>Helin Wang</B>, Yuexian Zou <BR>-->
<!--      Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification <BR>-->
<!--      Submitted to Interspeech 2021.-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->
<!--<UL>-->
<!--  <LI>-->
<!--  <P>Jinchuan Tian, Rongzhi Gu, <B>Helin Wang</B>, Yuexian Zou <BR>-->
<!--      Layer Reduction: Accelerating Conformer-Based Self-Supervised Model via Layer Consistency <BR>-->
<!--      Submitted to Interspeech 2021.-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->
<!--<UL>-->
<!--  <LI>-->
<!--  <P>Li Wang, Rongzhi Gu, <B>Helin Wang</B>, Yuexian Zou<BR>-->
<!--      Decoupling Feature Learning for Keyword Spotting and Speaker Verification via Orthogonal Convolution <BR>-->
<!--      Submitted to Interspeech 2021.-->
<!--   </P>-->
<!--  </LI>-->
<!--</UL>-->





<H2>Projects</H2>
<UL>
  <LI>
      <P><B>Member</B>: Research on Deep Analysis Method of Acoustic Scenes for Smart Home Robot <BR>
    The project is a Shenzhen Science and Technology Fundamental Research Program, which studies the acoustic scenes and events in real home environments, including robust acoustic feature extraction, acoustic scene classification methods, abnormal sound event detection and warning.<BR>
   </P>
  </LI>
</UL>

<UL>
  <LI>
      <P><B>Member</B>: Research on Multi-modal Health Monitoring System based on Infant Voices <BR>
    The project is a Shenzhen Science and Technology Fundamental Research Program, which studies the physiological characteristics of infant and conducts abnormal event detection based on audio and video signals. <BR>
   </P>
  </LI>
</UL>

<!-- <H2>Services</H2>
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"></div>
  </div> -->

<!-- <UL>
  <LI>
    <div style="float:left;">TASLP, Neurocomputing, Interspeech, ICASSP</div>
    <div style="float:right;">Reviewer (or PC Member)</div>
  </LI>
</UL> -->

<H2>Selected Awards</H2>
    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">1st Team Ranking of <A href="http://dcase.community/challenge2021/task-few-shot-bioacoustic-event-detection"> DCASE Challenge Task 5</A> <A href="http://dcase.community/challenge2019/awards"> (Judges’ award)</A> </div>
     <div style="float:right;">2021</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">4th Team Ranking of <A href="http://dcase.community/challenge2021/task-automatic-audio-captioning"> DCASE Challenge Task 6</A> <A href="http://dcase.community/challenge2019/awards"> (Judges’ award)</A> </div>
     <div style="float:right;">2021</div>
    </div>


    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">School Prize of Shanghai University</div>
     <div style="float:right;">2017-2018</div>
    </div>


    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">National Encouragement scholarship</div>
     <div style="float:right;">2017-2020</div>
    </div>
    
    <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">Bronze Prize of ACM-ICPC Asia Regional Competition</div>
      <div style="float:right;">2018</div>
     </div>

    <!-- <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">Bronze Prize of ACM-ICPC Asia Regional Competition</div>
      <div style="float:right;">2018</div>
     </div> -->


<div id="footer">
  <div id="footer-text"></div>
</div>
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fwanghelin1997.github.io%2Fhelinwang%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
<p><center>
<div id="clustrmaps-widget" style="width:50%">
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=QzXBa4ziRwuAQXpOVUeAtaD3JogYtR4qK4GIEVjmHNI&cl=ffffff&w=a"></script>
</div>
</center></p>
</div>

</DIV></DIV>
</BODY></HTML>
