<!-- --> 
<!-- saved from url=(0036)http://appsrv.cse.cuhk.edu.hk/~xchu/ -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><HTML 
xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><META 
content="IE=11.0000" http-equiv="X-UA-Compatible">

<META name="GENERATOR" content="MSHTML 11.00.9600.18125">
<META http-equiv="Content-Type" content="text/html;charset=utf-8">
<LINK href="style/jemdoc.css" rel="stylesheet" type="text/css">
<TITLE>Dongchao Yang</TITLE>
</HEAD>

<BODY>
<!--<div id="particles-js" class="particles-js"></div>-->
<!--<script src="./particles.js"></script>-->
<!--<script src="./app.js"></script>-->
<DIV id="layout-content">
<DIV id="make-narrow">
<DIV id="toptitle">

<H1>Dongchao Yang</H1></DIV>

<P>
<img src="img/dongchao.jpg" align="right" style="width:200px;height:210px;"> 
</P>

<P>SEEM<BR> The Chinese University of Hongkong, Hongkong</P>
<P><B>Email</B>: dcyang@se.cuhk.edu.hk</P>
<!-- <P><B>WeChat</B>: wxidbhl</P> -->
<p> 
<a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AJsN-F4czAkBmd4DO5WeMWN__2LqYg8Jay5asaDiB_UIfjsFyeVUj51geK3oGFEkutPWQ5_UGK3A0gZcH7mWzlAEoL09PJn3MUctPS9dKIedWhwSgg_mVfs&user=WNiojyAAAAAJ"><img src="./img/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
<a href="https://github.com/yangdongchao"><img src="./img/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
</p>
<!-- <P><A href="assets/cv.pdf">[Curriculum Vitae]</A></P> -->
<!-- <P><A href="https://dblp.uni-trier.de/pers/hd/b/Bai:Haoli">[DBLP]</A> -->
<!-- <A href="https://github.com/haolibai">[GitHub]</A> -->
<!--<a href="http://appsrv.cse.cuhk.edu.hk/~hlbai/CV_Haoli.pdf">[CV]</A>-->


<H2>General</H2>
    <P>I am a PhD student at The Chinese University of Hongkong, majoring in Speech and Audio Processing, Supervised by Prof. Helen Meng. Before that, I received the Master's Degree from Peking University in 2023. <BR>
      <BR>My research focus on developing a human-agent that can communicate with human, 
        e.g. understooding human's speech and environments sound, and then producing feedback to humans.<BR>
      <B>Note:</B> I am actively looking for any collaboration opportunities (e.g. Audio Foundation Models, Generative Models, TTS, Text-to-audio...). Please feel free to contact me.

<H2>Research Interests</H2>
<P>Audio Foundation Models, Generative Models, Large Language Models, Audio/Speech Processing</P>

<H2>Educations</H2>
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"></div>
  </div>

<UL>
  <LI>
    <div style="float:left;"> <A href=""> The Chinese University of Hongkong </A></div>
    <div style="float:right;">August 2023 - Now.</div>
  </LI>
</UL>


<UL>
  <LI>
    <div style="float:left;">School of Electronic and Computer Engineering, <A href="http://www.pku.edu.cn/"> Peking University</A></div>
    <div style="float:right;">August 2020 - August 2023.</div>
  </LI>
</UL>

<UL>
  <LI>
    <div style="float:left;">School of Computer Engineering and Science, <A href="https://cs.shu.edu.cn/index.htm"> Shanghai University</A></div>
    <div style="float:right;">August 2016 - July 2020.</div>
  </LI>
</UL>

<H2>Experiences</H2>
<UL>
  <LI>
  <P>July 2023 - Sep. 2023<BR>
      <B>MSRA</B>, <A href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Speech Group</A>, Intern. <BR> Supervisor: Xu Tan, <A href="https://www.microsoft.com/en-us/research/people/xuta/">  </A></A>
   </P>
  </LI>
</UL>
<UL>
  <LI>
  <P>May 2021 - May 2023<BR>
      <B>Tencent AI Lab</B>, <A href="https://ai.tencent.com/ailab/en/index"> Speech Group</A>, Intern. <BR> Supervisor: Songxiang Liu, <A href="https://scholar.google.com/citations?user=pRA19-8AAAAJ"></A> Chao Weng, <A href="https://scholar.google.com/citations?user=fY1IJ4wAAAAJ"></A>  and Bo Wu </A></A>
   </P>
  </LI>
</UL>


<H2>Selected Publications (refer to google scholar find all Publications)</H2>
<!-- <p>(Conference: 3 & Journal: 0)</p> -->

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, Dong Yu<BR>
    <A href="https://arxiv.org/pdf/2207.09983v1.pdf"> Diffsound: Discrete Diffusion Model for Text-to-sound generation</A><BR>
    <I>Accepted by IEEE Transactions on Audio, Speech and Language Processing.</I>, 2023.
    <A href="https://github.com/yangdongchao/Text-to-sound-Synthesis">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang*</B>, Songxiang Liu*, Rongjie Huang, Chao Weng, Helen Meng<BR>
    <A href="https://arxiv.org/pdf/2301.13662.pdf"> InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt</A><BR>
    <I>Accepted by IEEE Transactions on Audio, Speech and Language Processing </I>, 2024.
    <A href="">[https://github.com/yangdongchao/SoundStorm]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang*</B>, Jinchuan Tian*, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Helen Meng<BR>
    <A href="https://arxiv.org/pdf/2310.00704v5"> UniAudio: An Audio Foundation Model Toward Universal Audio Generation</A><BR>
    <I>ICML </I>, 2024.   <font color="#FF0000">Highligted by Artificial Intelligence Index Report 2024.</font> Only Three Audio related papers be highligted (MusicLM (Google), MusicGen (Meta), and Ours)  <A href="https://aiindex.stanford.edu/report/">[link]</A>
    <A href="https://github.com/yangdongchao/UniAudio">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, Helen Meng<BR>
    <A href="https://arxiv.org/pdf/2406.10056"> UniAudio 1.5: Large Language Model-driven Audio
      Codec is A Few-shot Audio Task Learner</A><BR>
    <I>NIPS </I>, 2024.
    <A href="https://github.com/yangdongchao/LLM-Codec">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Dingdong Wang, Haohan Guo, Xueyuan Chen, Xixin Wu, Helen Meng <BR>
    <A href="https://arxiv.org/pdf/2406.02328"> SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models </A><BR>
    <I>Proc. Interspeech</I>, 2024.  <font color="#FF0000"> ISCA Best Student Paper Award </font>
    <A href="">https://github.com/yangdongchao/SimpleSpeech</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>Rongjie Huang*, Jiawei Huang*, <B>Dongchao Yang*</B>, Yi Ren, et al. <BR>
    <A href="https://arxiv.org/pdf/2301.12661"> Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models</A><BR>
    <I>Accepted by ICML.</I>, 2023.
    <A href="">[https://github.com/Text-to-Audio/Make-An-Audio]</A>
   </P>
  </LI>
</UL>


<UL>
  <LI>
  <P><B> Dongchao Yang</B>, Helin Wang, Yuexian Zou, WenWu Wang <BR>
    <A href="https://arxiv.org/abs/2204.02088"> A Mixed Supervised Learning Framework for Target Sound Detection </A><BR>
      <I>DCASE Workshop</I>, 2022.
      <A href="https://github.com/yangdongchao/weakly-target-sound-detection">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>Rongjie Huang*, Mingzhe Li*, <B>Dongchao Yang*</B>, Jiatong Shi, et all<BR>
    <A href="https://arxiv.org/pdf/2304.12995"> AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</A><BR>
    <I> Accepted by AAAI 2024 </I>, 2023.
    <A href="https://github.com/AIGC-Audio/AudioGPT">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Helin Wang, Yuexian Zou <BR>
    <A href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/yang21b_interspeech.pdf"> Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification </A><BR>
    <I>Proc. Interspeech</I>, 2021.
    <A href="https://github.com/yangdongchao/interspeech2021_MTDA">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Helin Wang, Yuexian Zou, Zhongjie Ye, WenWu Wang <BR>
    <A href="https://arxiv.org/pdf/2110.04474.pdfhttps://arxiv.org/pdf/2110.04474.pdf"> A MUTUAL LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION
    </A><BR>
    <I>ICASSP</I>, 2022.
    <A href="https://github.com/yangdongchao/DCASE2021Task5">[Code]</A>
   </P>
  </LI>
</UL>

<H2>Preprints</H2>

<UL>
  <LI>
  <P><B>Dongchao Yang</B>, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, Xuexian Zou<BR>
    <A href="https://arxiv.org/abs/2305.02765"> HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec</A><BR>
    <I>Preprints </I>, 2023.
    <A href="">https://github.com/yangdongchao/AcademiCodec</A>
   </P>
  </LI>
</UL>



<H2>Services</H2>
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"></div>
  </div>

<UL>
  <LI>
    <div style="float:left;">IEEE/CAA Journal of Automatica Sinica</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 

<UL>
  <LI>
    <div style="float:left;">International Joint Conference on Artificial Intelligence (IJCAI) 2023</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 

<UL>
  <LI>
    <div style="float:left;">ICASSP, InterSpeech</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 

<UL>
  <LI>
    <div style="float:left;">ICML 2024, ICLR 2024, ACM-MM 2024</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 


<UL>
  <LI>
    <div style="float:left;">IEEE Transactions on Audio, Speech, and Language Processing</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 

<UL>
  <LI>
    <div style="float:left;">IEEE Transactions on Signal Processing Letter</div>
    <div style="float:right;">Reviewer</div>
  </LI>
</UL> 

<H2>Selected Awards</H2>

    <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">ISCA Best Student Paper Award</div>
      <div style="float:right;">2024</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">Outstanding graduate of Peking University</div>
      <div style="float:right;">2023</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">Excellent graduation thesis of Peking University</div>
      <div style="float:right;">2023</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">1st Team Ranking of <A href="http://dcase.community/challenge2021/task-few-shot-bioacoustic-event-detection"> DCASE Challenge Task 5</A> <A href="http://dcase.community/challenge2019/awards"> (Judgesâ€™ award)</A> </div>
     <div style="float:right;">2021</div>
    </div>
    
    <div style="overflow:hidden;margin-bottom: 0.6em">
      <div style="float:left;">Bronze Prize of ACM-ICPC Asia Regional Competition</div>
      <div style="float:right;">2018</div>
     </div>


<div id="footer">
  <div id="footer-text"></div>
</div>
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fwanghelin1997.github.io%2Fhelinwang%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
<p><center>
<div id="clustrmaps-widget" style="width:50%">
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=QzXBa4ziRwuAQXpOVUeAtaD3JogYtR4qK4GIEVjmHNI&cl=ffffff&w=a"></script>
</div>
</center></p>
</div>

</DIV></DIV>
</BODY></HTML>
